@page "/Machine Learning Pages/Support Vector Machines Part 1"
@inherits MathJaxContentComponent
<main>
    <article>
        <header>
            <h1>The Mathematics of Linear SVMs</h1>
            <h7>Prerequisites: Bias and Variance, Gradients and Partial Derivatives,
                and a basic understanding of SVMs (I recommend 
                <MudLink Href="https://youtu.be/efR1C6CvhmE" Underline="Underline.Hover" Style="font-size: inherit">Josh Starmer's StatQuest on the topic</MudLink>).
            </h7>
        </header>
        <br />
        <h3>Hard Margin SVMs</h3>
        <p style="display: inline">
            The goal of the hard margin SVM is to create as large of a margin as possible
            while not allowing for any misclassifications. The decision boundary equation is given as 
            $\mathbf{w}^{\top} \mathbf{x}_{i} + b = 0$, where $\mathbf{w}$ is the weight vector, $\mathbf{x}$ is the feature vector,
            and $b$ is the bias term/intercept. <div style="display: inline; cursor: pointer; color: rgb(89,74,226);" @onclick="() => isOpened1 = !isOpened1">[Why is $\mathbf{w}^{\top}$ used?]</div>
            @if (isOpened1)
            {
                <MLWebsite.Pages.Extras.WeightVectorTransposeExplanation></MLWebsite.Pages.Extras.WeightVectorTransposeExplanation>
            }
        </p>
        <br />
        <br />
        <p>Thus, it makes sense that the equation of the positive hyperplane is $\mathbf{w}^{\top} \mathbf{x} + b = 1$ (meaning anything that lies on this boundary
            or above it is placed into class A, labeled with 1) and that the equation of the negative hyperplane is $\mathbf{w}^{\top} \mathbf{x} + b = -1$ (meaning anything that lies on this boundary
            or below it is placed into class B, labeled with -1). Because $\mathbf{x}$ is already known, we want to find the values of $\mathbf{w}$ and $b$ such that the size of the margin is maximized.
        </p>
        <br />
        <p>However, is there a more specific task we can pursue to maximize this margin? We find that one does exist, but in order to understand why it makes sense, let us find a way to 
            express the margin of the SVM geometrically.  
        </p>
        <br />
        <p>
            Let $x_1$ be a vector lying on the positive hyperplane and $x_2$ be a vector lying on the negative hyperplane. This would mean that computing $x_1 - x_2$ would represent the width of the margin. 
            Thus, $\mathbf{w}^{\top}(x_1 - x_2) = 2$. To get rid of $\mathbf{w}^{\top}$, we need to divide both sides of the equation by the norm of $\mathbf{w}$, $|| \mathbf{w} ||$. Therefore, the width of the margin is $\frac{2}{|| \mathbf{w} ||}$. 
        </p>
        <br />
        <p>
            This means that we want to maximize the margin by minimizing $|| \mathbf{w} ||$. Instead of directly minimizing $|| \mathbf{w} ||$, however, we will instead minimize $\frac{1}{2} \mathbf{w}^{\top} \mathbf{w}$ (which is equivalent to $\frac{1}{2} || \mathbf{w} ||^{2}$).
            This is because it is easy to take the derivative of $\frac{1}{2} || \mathbf{w} ||^{2}$ because $|| \mathbf{w} ||$ is not differentiable at 0, making it much easier for optimization algorithms to work with the former function.
        </p>
        <br />
        <p style="display: inline">
            As stated earlier, when using a hard margin SVM, we want to make sure that there are no outliers, meaning that none of the vectors should lie within the margin. Numerically, each vector must be greater than or equal +1 or less than or equal to -1 to be on or past the hyperplanes and thus not an outlier.
            To include this with our results from earlier, we can define a variable $t_{i}$ where $t_{i} = +1$ for positive instances and $t_{i} = -1$ for negative instances. This would mean that $t_{i}$ multiplied by $\mathbf{w}^{\top} \mathbf{x}_{i} + b$ will result in a number that is greater than or equal to +1. 
            <div style="display: inline; cursor: pointer; color: rgb(89,74,226);" @onclick="() => isOpened2 = !isOpened2">[How does this work?]</div>
            @if (isOpened2)
            {
                <br />
                <MLWebsite.Pages.Extras.TiTermExplanation></MLWebsite.Pages.Extras.TiTermExplanation>
            }
        </p>
        <br />
        <br />
        <p>This gives us the objective for hard margin SVMs (also known as the <b>primal problem</b> for hard margin SVMs):</p>
        <p style="text-align: center">
            $$
            \underset{\mathbf{w}, b}{\operatorname{min}} \frac{1}{2} \mathbf{w}^{\top} \mathbf{w}
            $$
            $\text{subject to } t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i}+b) \geq 1 \text{ for } i=1,2, \cdots, m$
        </p>
        <br />
        <h3>Soft Margin SVMs</h3>
        <p>
            In order to get the objective for soft margin SVMs, we must introduce a <b>slack variable</b> $\zeta_{i} \geq 0$. This measures
            how much a particular instance is allowed to violate the margin (allowing soft margin SVMs to be distinguished from their hard margin counterparts). 
            Here's how it is defined:
            <br />
            <br />
            <ul>
                <li>If $\zeta_{i} = 0$, then this observation is on the correct side of the margin</li>
                <li>If $\zeta_{i} > 0$, then this observation has violated the margin and is on the wrong side of it (meaning on the wrong side of line $\mathbf{w}^{\top} \mathbf{x}_{i} + b = 0$),
                    but is not on or past the incorrect hyperplane (again, the edges of the margin)</li>
                <li>If $\zeta_{i} > 1$, then this observation is on or past the incorrect hyperplane</li>
            </ul>
        </p>
        <br />
        <p>
            This introduces conflicting objectives: making the slack variables as small as possible (and thus reducing the margin violations) and making $\frac{1}{2} \mathbf{w}^{\top} \mathbf{w}$ as small as possible (in order to maximize the margin).
            The <b>C hyperparameter</b> is used to define the tradeoff between these two objectives. When C is large, we seek narrow margins that are violated less often, resulting in a model that is more fit
            to those specific data instances (lower bias, higher variance). When C is small, we seek wider margins but allow more violations, resulting in a model that fits the data less tightly but may generalize better across different datasets (higher bias, lower variance).
        </p>
        <br />
        <p>
            With this in mind, we can define the primal problem for soft margin SVMs:
        </p>
        <p style="text-align: center">
            $$
            \underset{\mathbf{w}, b, \zeta}{\operatorname{min}} \frac{1}{2} \mathbf{w}^{\top} \mathbf{w}+C \sum_{i=1}^{m} \zeta_{i}
            $$
            $\text{subject to } t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i} + b) \geq 1 - \zeta_{i} \text{ and } \zeta_{i} \geq 0 \text{ for } i = 1, 2, \cdots, m$
        </p>
        <p> 
            The reason $\sum\limits_{i=1}^{m} \zeta_{i}$ is added at the end is because if it did not exist, we could choose a very large value of $\zeta_{i}$ and satisfy $t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i} + b) \geq 1 - \zeta_{i}$ without being penalized.
        </p>
        <br />
        <h3>Deriving the Dual Problem</h3>
        <p>Before deriving the dual problem, we need to discuss the duality principle in optimization and review the method of Lagrange multipliers.</p>
        <br />
        <p>
            The duality principle states that optimization problems can viewed as either the <b>primal problem</b> or the <b>dual problem</b>, with the difference between them being that the primal problem is a minimization problem, while the dual problem is a maximization problem. A property of the dual problem is that it provides a lower bound to the solution of the primal problem, meaning
            its solution (a maximum) will always be less than or equal to the solution of the primal problem (a minimum).
        </p>
        <br />
        <p>
            Now, we'll briefly review solving constrained optimization problems using the method of Lagrange multipliers. If we want to maximize or minimize a function $f(x)$ subject to 
            some equality constraint given by $g(x)$, we can form the <b>Lagrangian function</b>, often shortened to <b>Lagrangian</b>:
            $$\mathcal{L} (x, \alpha) = f(x) - \alpha g(x)$$
            where $\alpha$ is a <b>Lagrange multiplier</b>. Then, we can set the gradient of the Lagrangian to 0 and find the points where all the partial derivatives of the
            Lagrangian with respect to each variable are equal to 0 (known as <b>stationary points</b>). The solution(s) to the constrained optimization problem must be among the stationary points, if they exist at all.
        </p>
        <br />
        <p>
            What makes this method so powerful is that it allows the constraint to be incorporated with the function that is being optimized into the separate and unconstrained Lagrangian. Computers can very easily solve for the stationary points and find
            the solutions to the problem from there. Now that we have reviewed both duality and the method of Lagrange multipliers, we can use it in the context of SVMs.
        </p>
        <br />
        <p>
            Let us begin by looking at the hard margin SVM problem and the <b>generalized Lagrangian for the hard margin SVM problem</b> (using the primal problem that was presented previously): 
        </p>
        <p style="text-align: center">
            $$\mathcal{L} (\mathbf{w}, b, \alpha) = \frac{1}{2} \mathbf{w}^{\top} \mathbf{w}-\sum\limits_{i=1}^{m} \alpha_{i} (t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i}+b)-1)$$
            $\text{subject to } \alpha_{i} \geq 0 \text{ for } i=1,2, \cdots, m$
        </p>
        <br />
        <p>
            Incorporating the minimization from the primal problem, we derive the following Lagragian problem:
        </p>
        <p style="text-align: center">
            $$\underset{\mathbf{w}, b}{\operatorname{min}} \underset{\alpha}{\operatorname{max}} \mathcal{L} (\mathbf{w}, b, \alpha)$$
            $\text{subject to } \alpha_{i} \geq 0 \text{ for } i=1,2, \cdots, m$
        </p>
        <br />
        <p>
            This means that we must minimize the Lagrangian with respect to $\mathbf{w}$ and $b$ while also maximizing it with respect to $\alpha$.
            <div style="display: inline; cursor: pointer; color: rgb(89,74,226);" @onclick="() => isOpened3 = !isOpened3">[Why is this true?]</div>
            @if (isOpened3)
            {
                <MLWebsite.Pages.Extras.MaximizingAlphaExplanation></MLWebsite.Pages.Extras.MaximizingAlphaExplanation>
            }
        </p>
        <br />
        <p>
           You may notice an issue in using method of Lagrange multipliers for this problem: in our discussion about the method of Lagrange multipliers, we considered an optimization problem with equality constraints, but notice that the 
           hard margin SVM problem instead had inequality constraints. To resolve this issue, we look to the <b>Karush-Kuhn-Tucker (KKT) conditions</b>.
        </p>
        <br />
        <p>
            The KKT conditions are first-order necessary conditions (equivalent to first derivative tests) for an optimal solution of an optimization problem, given that some regularity conditions are met (cite Wikipedia here).
            One of these regularity conditions is <b>Slater's condition</b>, which holds for affine constraints (Gretton qtd. in Kowalczyk 56). This states that strong duality holds for this problem, meaning 
            the solution to the dual problem is equal to that of the primal problem. Slater's condition holds for the SVM optimization problems, and because the primal problems for SVMs are convex ones, if a solution satisfies the KKT conditions, it is 
            the optimal solution to our problem. Now, let us look at each of the KKT conditions in the context of the Lagrangian presented earlier:
            <br />
            <br />
            <ul style="padding-left: 10% !important;">
                <li>
                    Stationarity Condition:
                    $$\nabla_{\mathbf{w}} \mathcal{L}=\mathbf{w}-\sum\limits_{i=1}^{m} \alpha_{i} t_{i} \mathbf{x}_{i}=\mathbf{0}$$
                    $$\frac{\partial \mathcal{L}}{\partial b}=-\sum\limits_{i=1}^{m} \alpha_{i} t_{i}=0$$
                    Note that $\nabla_{\mathbf{w}} \mathcal{L}$ means taking the partial derivative of $\mathcal{L}$ with respect to vector $\mathbf{w}$.
                    This condition tells us that a solution must be a stationary point, meaning a point where the partial derivatives of the Lagrangian with respect to $\mathbf{w}$ and $b$ are 0 (where the gradient of the Lagrangian is 0).
                    <br />
                    <br />
                </li>
                <li>
                    Prime Feasibility Condition:
                    $$t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i} + b) -1 \geq 0$$
                    $$\text{for } i=1,2, \cdots, m$$
                    All this condition states is that the constraints of the primal problem are to be respected.
                    <br />
                    <br />
                </li>
                <li>
                    Dual Feasibility Condition:
                    $$\alpha_{i} \geq 0$$
                    $$\text{for } i=1,2, \cdots, m$$
                    This condition is very similar to the primal feasibility condition, stating that the constraints of the dual problem (which will be presented in a moment) are to be respected.
                    <br />
                    <br />
                </li>
                <li>
                    Complementary Slackness Condition:
                    $$\alpha_{i}[t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i}+b) -1]=0$$
                    $$\text{ for } i=1,2, \cdots, m$$
                    This condition has a relatively intuitive explanation. This equation illustrates that there are two ways for this condition to be satisfied: either $\alpha_{i} = 0$ or $t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i} + b) - 1 = 0$.
                    We also know that when a feature vector $\mathbf{x}_{i}$ is classified correctly, $t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i} + b) - 1 > 0$. For these points, $t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i} + b) - 1 \neq 0$, so in order for
                    the complementary slackness condition to be true, $\alpha_{i} = 0$. Thus, we can say that $\alpha_{i} = 0$ for every correctly classified vector. This also means that $t_{i}(\mathbf{w}^{\top} \mathbf{x}_{i} + b) - 1 = 0$ for points that lie on the
                    either of the hyperplanes $\mathbf{w}^{\top} \mathbf{x} + b = 1$ and $\mathbf{w}^{\top} \mathbf{x} + b = -1$, so vectors that lie on either of those hyperplanes have a positive Lagrange multiplier $\alpha_{i}$. These vectors are called <b>support vectors</b>.
                </li>
            </ul>
        </p>
        <br />
        <p>
            Now, we can derive the dual problem for the hard margin SVM. If we rearrange $\nabla_{\mathbf{w}} \mathcal{L}=\mathbf{w}-\sum\limits_{i=1}^{m} \alpha_{i} t_{i} \mathbf{x}_{i}=\mathbf{0}$, we can find that the following is true of $\mathbf{w}$:
            $$\mathbf{w} = \sum\limits_{i=1}^{m} \alpha_{i} t_{i} \mathbf{x}_{i}$$
            Let us now substitute every $\mathbf{w}$ in the Lagrangian with $\sum\limits_{i=1}^{m} \alpha_{i} t_{i} \mathbf{x}_{i}$:
            $$
            \begin{aligned}
            W(\alpha, b)=\frac{1}{2}\left(\sum_{i=1}^{m} \alpha_{i} t_{i} \mathbf{x}_{i}\right) \cdot\left(\sum_{j=1}^{m} \alpha_{j} t_{j} \mathbf{x}_{j}\right)-\sum_{i=1}^{m} \alpha_{i}\left[t_{i}\left(\left(\sum_{j=1}^{m} \alpha_{j} t_{j} \mathbf{x}_{j}\right) \cdot \mathbf{x}_{i}+b\right)-1\right] \\
            =\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} t_{i} t_{j} \mathbf{x}_{i} \cdot \mathbf{x}_{j}-\sum_{i=1}^{m} \alpha_{i} t_{i}\left(\left(\sum_{j=1}^{m} \alpha_{j} t_{j} \mathbf{x}_{j}\right) \cdot \mathbf{x}_{i}+b\right)+\sum_{i=1}^{m} \alpha_{i} \\
            =\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} t_{i} t_{j} \mathbf{x}_{i} \cdot \mathbf{x}_{j}-\sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} t_{i} t_{j} \mathbf{x}_{i} \cdot \mathbf{x}_{j}-b \sum_{i=1}^{m} \alpha_{i} t_{i}+\sum_{i=1}^{m} \alpha_{i} \\
            =\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} t_{i} t_{j} \mathbf{x}_{i}^{\top} \cdot \mathbf{x}_{j}-b \sum_{i=1}^{m} \alpha_{i} t_{i}
            \end{aligned}
            $$
            <br />
            We can do something similar with $b$. However, given that $\frac{\partial \mathcal{L}}{\partial b}=-\sum\limits_{i=1}^{m} \alpha_{i} t_{i}=0$, if we see $\sum\limits_{i=1}^{m} \alpha_{i} t_{i}$, we can substitute it with 0 and simplify from there. We can see that the term exists on the right-hand side of the 
            function, meaning we can simplify it to the following:
            $$
            W(\alpha) = \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} t_{i} t_{j} \mathbf{x}_{i}^{\top} \cdot \mathbf{x}_{j}
            $$
            This function is known as the <b>Wolfe dual Lagrangian function</b>, and we can now use this to define the <b>Wolfe dual problem for hard margin SVMs</b>:
        </p>
        <p style="text-align: center">
            $$
            \underset{\alpha}{\operatorname{max}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} t_{i} t_{j} \mathbf{x}_{i}^{\top} \cdot \mathbf{x}_{j}
            $$
            $$\text{subject to } \alpha_{i} \geq 0 \text{ for } i=1,2, \cdots, m$$
            $$\sum\limits_{i=1}^{m} \alpha_{i} t_{i} = 0$$
        </p>
        <p>
            Normally, the Wolfe dual Lagrangian problem is constrained by setting the gradient to 0 (meaning each partial derivative is set to 0). The only reason we added $\sum\limits_{i=1}^{m} \alpha_{i} t_{i} = 0$ is because 
            we need it to get rid of $b$ from the function. However, applying the constraint $\mathbf{w} = \sum\limits_{i=1}^{m} \alpha_{i} t_{i} \mathbf{x}_{i}$ is not necessary because we can solve the problem without it.
        </p>
        <br />
        <br />
        <p>
            We can now also define the <b>Wolfe dual problem for soft margin SVMs</b>:
        </p>
        <p style="text-align: center">
            $$
            \underset{\alpha}{\operatorname{max}} \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} t_{i} t_{j} \mathbf{x}_{i}^{\top} \cdot \mathbf{x}_{j}
            $$
            $$\text{subject to } 0 \leq \alpha_{i} \leq C \text{ for } i=1,2, \cdots, m$$
            $$\sum\limits_{i=1}^{m} \alpha_{i} t_{i} = 0$$
        </p>
        <p>
            The only difference between this dual problem and the hard margin dual problem is that $\alpha_{i}$ must be less than or equal to $C$. The introduction of this constraint
            is necessary because, without slack, $\alpha_{i}\to+\infty$ when the constraints are violated, which occurs when points are misclassified. Enforcing $C$ as the upper bound limits $\alpha_{i}$ such that misclassifications are possible. This also illustrates
            how changing the value of $C$ can change the number of misclassifications allowed and how the margin is constructed. 
        </p>
        <br />
        <h3>Computing $\mathbf{w}$ and $b$</h3>
        <p>
            Solving the Wolfe dual problem gives us a vector $\alpha$ of all Lagrange multipliers, and because the original goal of the primal problem was to find $\mathbf{w}$ and $b$, we can use $\alpha$ to compute both.
            Finding $\mathbf{w}$ is straightforward, given that we know that $\mathbf{w} = \sum\limits_{i=1}^{m} \alpha_{i} t_{i} \mathbf{x}_{i}$. Once we find $\mathbf{w}$, we can use the primal problem constraint $t_{i}(\mathbf{w}^{\top} \mathbf{x} + b) - 1 \geq 0$ to find $b$.
            Specifically, we can consider some $\mathbf{x}_{i}$ that lies on either of the hyperplanes, which would give us the equation $t_{i} (\mathbf{w}^{\top} \mathbf{x}_{i} - b) = 1$, which can be rearranged into $b = t_{i} - \mathbf{w}^{\top} \mathbf{x}_{i}$ (remember that $t_{i} = \pm 1$, which means that $\frac{1}{t_{i}} = t_{i}$).
            However, this is typically not the equation that is used. Instead, it is common to use the average of this value over all support vectors rather than just 1, giving us the following formula to compute $b$: 
            $$b = \frac{1}{n}\sum_{\substack{i=1 \\ \alpha_{i} > 0}}^{m} (t_{i} - \mathbf{w}^{\top} \mathbf{x}_{i})$$
        </p>
        <h3>What's Next?</h3>
        <p>
            Both hard margin and soft margin SVMs assume that the best way to classify a dataset is using a line. But what happens when data is not linearly separable? This is where <b>kernels</b> become useful. We will discuss this
            in <a href="/Machine Learning Pages/Support Vector Machines Part 2" style="color: rgb(89,74,226)">Kernelized SVMs</a>.
        </p>
    </article>
</main>

@code {
    bool isOpened1;
    bool isOpened2;
    bool isOpened3;
}
